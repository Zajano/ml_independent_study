{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMeans_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "source": [
        "# Score Prediction Regression Model"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "UID9RK1qDlVB",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO_1kOEGDTws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "eccc66bd-8b04-4d4f-faa1-b4b0d22df5c8"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "os.chdir(r\"C:\\Users\\Zack\\Desktop\\work\\OSU\\406 - p2 - learning\\jupyter practice\\final models\")\n",
        "game_data = pd.read_csv('game_data.csv') # already filtered buy num_reviews >= 30\n",
        "# game_data.head() # See the first 5 rows"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_pPmK9GIKMz",
        "colab_type": "text"
      },
      "source": [
        "### clean and filter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WevSKogFEalU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "dc31b65e-b7f5-4116-ec89-5790fdb604bc"
      },
      "source": [
        "# bgg_games = game_data[game_data['type'] == 'boardgame'] # no expansions\n",
        "# bgg_games = bgg_games[bgg_games['year'] > 1980]\n",
        "bgg_games = game_data[game_data['year'] > 1980]\n",
        "bgg_games = bgg_games[bgg_games['maxplayers'] <= 30]\n",
        "bgg_games = bgg_games[bgg_games['minplaytime'] <= 180] # 120 - 90th percentile\n",
        "bgg_games = bgg_games[bgg_games['maxplaytime'] <= 720]\n",
        "bgg_games = bgg_games[bgg_games['minage'] <= 21]\n",
        "bgg_games = bgg_games[bgg_games['playingtime'] >= 10]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cell for data exploration\n",
        "# bgg_games.columns"
      ]
    },
    {
      "source": [
        "### select cells potentially relevant to rating (before community interaction)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtc_test = bgg_games[['type', 'minplayers', 'maxplayers', 'playingtime',\n",
        "       'minplaytime', 'maxplaytime', 'minage', 'avg_rating', 'mechanics',\n",
        "       'bay_rating', 'total_comments', 'total_weights', 'complexity', 'categories']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for player pool size\n",
        "dtc_test = dtc_test[dtc_test['maxplayers'] >= dtc_test['minplayers']]"
      ]
    },
    {
      "source": [
        "#### convert mechanics and categories into lists with values"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtc_test['categories'] = dtc_test['categories'].apply(lambda x: x.strip('][').split(', ') )\n",
        "dtc_test['mechanics'] = dtc_test['mechanics'].apply(lambda x: x.strip('][').split(', ') )"
      ]
    },
    {
      "source": [
        "#### count number of mechanics and categories for each game, make new columns"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_mechs = []\n",
        "num_cats = []\n",
        "for index, row in dtc_test.iterrows():\n",
        "    num_mechs.append(len(row['mechanics']))\n",
        "    num_cats.append(len(row['categories']))\n",
        "\n",
        "dtc_test['num_mechs'] = num_mechs\n",
        "dtc_test['num_cats'] = num_cats\n",
        "dtc_test['player_diff'] = dtc_test.maxplayers - dtc_test.minplayers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    20905.000000\n",
              "mean        64.891701\n",
              "std         54.955134\n",
              "min         10.000000\n",
              "25%         30.000000\n",
              "50%         45.000000\n",
              "75%         90.000000\n",
              "max        720.000000\n",
              "Name: playingtime, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# explore data\n",
        "dtc_test['playingtime'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30     3722\n",
              "60     3703\n",
              "45     2489\n",
              "120    2237\n",
              "90     2227\n",
              "       ... \n",
              "23        1\n",
              "38        1\n",
              "165       1\n",
              "68        1\n",
              "95        1\n",
              "Name: playingtime, Length: 65, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# data exploration\n",
        "dtc_test['playingtime'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# data exploration\n",
        "dtc_test.loc[dtc_test.playingtime < 10, 'playingtime'].count()\n",
        "\n",
        "# dtc_test['time_diff'] = dtc_test.maxplaytime - dtc_test.minplaytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_data_frame_list(df, target_column, output_type=str):\n",
        "    ''' \n",
        "    Accepts a column with list values and splits into several rows.\n",
        "\n",
        "    df: dataframe to split\n",
        "    target_column: the column containing the values to split\n",
        "    output_type: type of all outputs\n",
        "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
        "    The values in the other columns are duplicated across the newly divided rows.\n",
        "    '''\n",
        "    row_accumulator = []\n",
        "\n",
        "    def split_list_to_rows(row):\n",
        "        split_row = row[target_column]\n",
        "        if isinstance(split_row, list):\n",
        "          for s in split_row:\n",
        "              new_row = row.to_dict()\n",
        "              new_row[target_column] = output_type(s)\n",
        "              row_accumulator.append(new_row)\n",
        "        else:\n",
        "          new_row = row.to_dict()\n",
        "          new_row[target_column] = output_type(split_row)\n",
        "          row_accumulator.append(new_row)\n",
        "  \n",
        "    df.apply(split_list_to_rows, axis=1)\n",
        "    new_df = pd.DataFrame(row_accumulator)\n",
        "  \n",
        "    return new_df"
      ]
    },
    {
      "source": [
        "#### split lists into multiple rows for regression model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtc_test = split_data_frame_list(dtc_test, 'categories')\n",
        "dtc_test = split_data_frame_list(dtc_test, 'mechanics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(dtc_test.dtypes)"
      ]
    },
    {
      "source": [
        "#### get all desired cols and apply one-hot fix to categorical features"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# for filtering cols\n",
        "desired_cols = ['type', 'minplayers', 'maxplayers', 'playingtime', 'minplaytime', 'maxplaytime', 'minage', 'avg_rating', 'bay_rating', 'complexity', 'categories', 'mechanics', 'num_mechs', 'num_cats', 'player_diff']\n",
        "\n",
        "# make dummies (one-hot fix) for categorical values\n",
        "# will remove categorical columns\n",
        "model_frame = dtc_test[desired_cols]\n",
        "model_frame = pd.get_dummies(model_frame, drop_first=True)"
      ]
    },
    {
      "source": [
        "#### filter out categorical columns for tree fitting"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# list of just desired features, now including one-hot cols\n",
        "features = list(model_frame.columns)\n",
        "\n",
        "# remove target cols\n",
        "features.remove('avg_rating')\n",
        "features.remove('bay_rating')\n",
        "\n",
        "# for col in desired_cols:\n",
        "#     features.remove(col)\n",
        "# print(features)"
      ]
    },
    {
      "source": [
        "### Divide the data set\n",
        "#### split data into training portions"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n"
          ]
        }
      ],
      "source": [
        "# filtered features with one-hot fixes for categorical columns\n",
        "X = model_frame[features]\n",
        "\n",
        "# remove undesired columns\n",
        "X.drop(\"categories_'Expansion for Base-game'\", inplace=True, axis=1) # duplicate of \"is expansion\"\n",
        "\n",
        "# change rating to int value for library methods\n",
        "# model_frame['avg_rating'] = model_frame['avg_rating'].apply(lambda x: int(round(100000 * x, 0)))\n",
        "\n",
        "# target variable - bays because it's not as dramatic\n",
        "y = model_frame[['avg_rating']]\n",
        "\n",
        "# Split method, 0.3 == 30% of data saved for testing, choosen randomly from set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0001, random_state=1)\n",
        "print('done!')"
      ]
    },
    {
      "source": [
        "### feature selection"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from functools import partial\n",
        "\n",
        "# f_regression is univarite - direct correlations\n",
        "# mutual_info compares multiple feature pairs\n",
        "\n",
        "def feature_selection(X_train, y_train, m=0, n='all'):\n",
        "\t'''produces feature values to help with selection\n",
        "\tIN: 3 frames split from data\n",
        "\t\tm = type of method desired (int)\n",
        "\t\tn = number of top features to select\n",
        "\tOUT: transformed X-data and feature selection model'''\n",
        "\t# partial to establish params for mutual info\n",
        "\t# CAN'T HANDLE\n",
        "\tmutual_info = partial(mutual_info_regression, random_state=0)\n",
        "\n",
        "\t# scoring functions to use\n",
        "\tmethods = [f_regression, mutual_info]\n",
        "\t# configure to select all features\n",
        "\tfs = SelectKBest(score_func=methods[m], k=n)\n",
        "\t# learn relationship from training data\n",
        "\tfs.fit(X_train, y_train)\n",
        "\t# # transform train input data\n",
        "\t# X_train_fs = fs.transform(X_train)\n",
        "\t# # transform test input data\n",
        "\t# X_test_fs = fs.transform(X_test)\n",
        "\t# return feature selection model and scores\n",
        "\treturn fs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n"
          ]
        }
      ],
      "source": [
        "# feature selection scores (currently set to check all features)\n",
        "# compare mutual selection values to univariate values\n",
        "# arbitrarily chose '100' as the f_value cutoff for desireed features\n",
        "fs = feature_selection(X_train, y_train, 0, 15)\n",
        "\n",
        "# BAD IDEA - don't turn a countinuous value (rating float) into 'multiclass' representation with integers!\n",
        "# X_train_fs_mut, X_test_fs_mut, fs_mut = feature_selection(X_train, y_train, X_test, 2)\n",
        "\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "278\n"
          ]
        }
      ],
      "source": [
        "print(len(list(X.columns)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apply top feature to new DF for model\n",
        "feature_mask = fs.get_support()\n",
        "top_features = X.columns[feature_mask]\n",
        "# print(top_features)"
      ]
    },
    {
      "source": [
        "### reset test data for mutual information filtering"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# NOPE!"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = model_frame[top_features]\n",
        "\n",
        "# # still the same\n",
        "# y = model_frame[['avg_rating']]\n",
        "\n",
        "# # 0 - find correlations with all data available\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0001, random_state=2)\n",
        "# print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "Unable to allocate 356. KiB for an array with shape (45581,) and data type int64",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-55-224e875f8f1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# new feature selection model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfs_mut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-39-0318665c1fbd>\u001b[0m in \u001b[0;36mfeature_selection\u001b[1;34m(X_train, y_train, m, n)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# learn relationship from training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;31m# # transform train input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# X_train_fs = fs.transform(X_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36mmutual_info_regression\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    366\u001b[0m            \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mRandom\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mProbl\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mPeredachi\u001b[0m \u001b[0mInf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m23\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1987\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \"\"\"\n\u001b[1;32m--> 368\u001b[1;33m     return _estimate_mi(X, y, discrete_features, False, n_neighbors,\n\u001b[0m\u001b[0;32m    369\u001b[0m                         copy, random_state)\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1e-10\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n\u001b[0m\u001b[0;32m    289\u001b[0m           x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1e-10\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n\u001b[0m\u001b[0;32m    289\u001b[0m           x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_compute_mi\u001b[1;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compute_mi_cd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compute_mi_cc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_compute_mi_cc\u001b[1;34m(x, y, n_neighbors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mradius_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mnx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mradius_neighbors\u001b[1;34m(self, X, radius, return_distance, sort_results)\u001b[0m\n\u001b[0;32m    994\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mind_neighbor\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 996\u001b[1;33m                 \u001b[0mneigh_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mind_neighbor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    997\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m                     \u001b[0mneigh_dist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneigh_dist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 356. KiB for an array with shape (45581,) and data type int64"
          ]
        }
      ],
      "source": [
        "# new feature selection model\n",
        "fs_mut = feature_selection(X_train, y_train, 1, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# feature_mask = fs_mut.get_support()\n",
        "# top_features = X.columns[feature_mask]\n",
        "# print(top_features)"
      ]
    },
    {
      "source": [
        "## reset test data with new features that were selected"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = model_frame[top_features]\n",
        "\n",
        "# still the same\n",
        "y = model_frame[['avg_rating']]\n",
        "\n",
        "# Split method, 0.3 == 30% of data saved for testing, choosen randomly from set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)"
      ]
    },
    {
      "source": [
        "### Exploration of stats for feature values"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get features only with a p-value < 0.05 (indicates it has an effect on target variable)\n",
        "# scores = []\n",
        "# big_scores = []\n",
        "\n",
        "# for i in range(len(fs.pvalues_)):\n",
        "#     if fs.pvalues_[i] < 0.05:\n",
        "#         # tuple of feature number and feature value\n",
        "#         scores.append((i, fs.scores_[i]))\n",
        "#         if fs.scores_[i] >= 100:\n",
        "#             big_scores.append((i, fs.scores_[i]))\n",
        "#     # same check for mutual values\n",
        "#     # if fs_mut.pvalues_[i] < 0.05:\n",
        "#     #     # tuple of feature number and feature value\n",
        "#     #     mut_scores.append((i, fs_mut.scores_[i]))\n",
        "# print(len(scores))\n",
        "# print(len(big_scores)) # 68 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # remove ouliers to prep for getting standard deviation, then feature trimming\n",
        "# score_nums = sorted(list(list(zip(*big_scores))[1]))\n",
        "# # quartile splits of data, and range of difference\n",
        "# q1, q3 = np.percentile(score_nums, [25, 75])\n",
        "# iqr = q3 - q1\n",
        "# # bounds to trim outliers\n",
        "# low_bound = q1 - (1.5 * iqr)\n",
        "# up_bound = q3 + (1.5 * iqr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import statistics as stats\n",
        "# import bisect\n",
        "\n",
        "# # get indices of where to trim outliers\n",
        "# idx_left = bisect.bisect_right(score_nums, low_bound)\n",
        "# idx_right = bisect.bisect_left(score_nums, up_bound)\n",
        "\n",
        "# # trim outliers and get math\n",
        "# timmed_nums = score_nums[idx_left:idx_right]\n",
        "# scores_std = stats.stdev(timmed_nums)\n",
        "# scores_mean = stats.mean(timmed_nums)\n",
        "# print(scores_std, \" \", scores_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # scores for the features\n",
        "# scores = []\n",
        "# for i in range(len(fs.scores_)):\n",
        "# \t# print('Feature %d: %f' % (i, fs.scores_[i]))\n",
        "# \tscores.append((i, fs.scores_[i]))\n",
        "\n",
        "# sort by feature value\n",
        "# scores.sort(key = lambda x: x[1])\n",
        "# plot the scores\n",
        "# plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
        "# plt.show()"
      ]
    },
    {
      "source": [
        "### Train the model "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137767\n"
          ]
        }
      ],
      "source": [
        "# X_train.describe()\n",
        "print(len(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q42-XPJjIyXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "94c92518-3ea7-48e3-999c-762159548e53"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from copy import deepcopy\n",
        "import math\n",
        "\n",
        "\n",
        "best_r2 = 0\n",
        "best_mse = math.inf\n",
        "\n",
        "# for i in range(10,20):\n",
        "    # Regression Model objects\n",
        "    # lgm = LinearRegression()\n",
        "    # min_samples_leaf=5\n",
        "rfr = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42)\n",
        "\n",
        "# fit regression models\n",
        "# lgm.fit(X_train,y_train)\n",
        "rfr.fit(X_train,y_train)\n",
        "\n",
        "# predictions by model for y\n",
        "# y_pred_L = lgm.predict(X_test)\n",
        "y_pred_F = rfr.predict(X_test)\n",
        "\n",
        "# accuracy check, lower is better\n",
        "# mse_L = mean_squared_error(y_test, y_pred_L)\n",
        "mse_F = mean_squared_error(y_test, y_pred_F)\n",
        "# print('MSE linear: ', mse_L)\n",
        "# print('MSE Forest: ', mse_F)\n",
        "\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "# r2_L = r2_score(y_test, y_pred_L)\n",
        "r2_F = r2_score(y_test, y_pred_F)\n",
        "# print('r2 Linear: ', r2_L)\n",
        "print('r2 Forest: ', r2_F)\n",
        "\n",
        "    # if r2_F > best_r2:\n",
        "    #     best_lgm = deepcopy(lgm)\n",
        "    #     best_r2 = r2_F\n",
        "    #     best_mse = mse_F\n",
        "    #     print(\"mse: \", best_mse)\n",
        "    #     print(\"r2: \", best_r2)\n",
        "\n",
        "print('done!')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r2 Forest:  0.9411821686719545\ndone!\n"
          ]
        }
      ]
    },
    {
      "source": [
        "### save best model and test accuracy of accuracy rating"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE:  0.050877176437359596\nr2:  0.9411821686719545\n"
          ]
        }
      ],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "# swtich to model directory\n",
        "os.chdir(r\"C:\\Users\\Zack\\Desktop\\work\\OSU\\406 - p2 - learning\\jupyter practice\\final models\\models\")\n",
        "\n",
        "# create and save file\n",
        "joblib_file = \"rfr_r29424_mse0498.joblib\"  \n",
        "dump(rfr, joblib_file)\n",
        "\n",
        "# test load\n",
        "joblib_model = load(joblib_file)\n",
        "\n",
        "y_pred = joblib_model.predict(X_test)\n",
        "\n",
        "# accuracy check\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('MSE: ', mse)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('r2: ', r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_rating    8.22286\nName: 42866, dtype: float64\n8.218860365220493\n"
          ]
        }
      ],
      "source": [
        "# check specific instances for accuracy\n",
        "print(y_test.iloc[170])\n",
        "print(y_pred[170])"
      ]
    },
    {
      "source": [
        "### visualize training depths"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "max_depth = []\n",
        "acc_gini = []\n",
        "acc_entropy = []\n",
        "\n",
        "best_acc = correct / len(y_pred)\n",
        "best_dtc = dtc\n",
        "\n",
        "y_targets = y_test[\"categories\"].tolist() \n",
        "for i in range(1,36):\n",
        "    # testing entropy\n",
        "    dtree = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=i)\n",
        "    dtree.fit(X_train, y_train)\n",
        "    y_pred = dtree.predict(X_test)\n",
        "    correct = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        if y_pred[j] in y_targets[j]:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(y_pred)\n",
        "    acc_entropy.append(accuracy)\n",
        "    if accuracy > best_acc:\n",
        "        best_dtc = deepcopy(dtree)\n",
        "        best_acc = accuracy\n",
        "        print(\"best acc: \", accuracy)\n",
        "\n",
        "    # testing gini\n",
        "    dtree = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=i)\n",
        "    dtree.fit(X_train, y_train)\n",
        "    y_pred = dtree.predict(X_test)\n",
        "    correct = 0\n",
        "    for j in range(len(y_pred)):\n",
        "        if y_pred[j] in y_targets[j]:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(y_pred)\n",
        "    acc_gini.append(accuracy)\n",
        "\n",
        "    if accuracy > best_acc:\n",
        "        best_dtc = deepcopy(dtree)\n",
        "        best_acc = accuracy\n",
        "        print(\"best acc: \", accuracy)\n",
        "\n",
        "    # track depth for values\n",
        "    max_depth.append(i)\n",
        "\n",
        "\n",
        "# data frame with tracked values to graph\n",
        "df = pd.DataFrame({'acc_gini':pd.Series(acc_gini), \n",
        "'acc_entropy':pd.Series(acc_entropy),\n",
        "'max_depth':pd.Series(max_depth)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib_file = \"dtc_8936.joblib\"  \n",
        "dump(best_dtc, joblib_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# graph folder\n",
        "os.chdir(r\"C:\\Users\\Zack\\Desktop\\work\\OSU\\406 - p2 - learning\\jupyter practice\\final models\\graphs\")\n",
        "\n",
        "#size \n",
        "sns.set(rc={'figure.figsize': (6, 6)})\n",
        "\n",
        "print(max_depth)\n",
        "# visualizing changes in parameters\n",
        "sns.lineplot(x='max_depth', y='acc_gini', data=df)\n",
        "sns.lineplot(x='max_depth', y='acc_entropy', data=df)\n",
        "plt.xlabel('max depth')\n",
        "plt.ylabel('accuracy')\n",
        "# plt.xlim(1,30)\n",
        "plt.savefig('gini vs entropy', bbox_inches = 'tight')\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "### Visualize training tree model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "from six import StringIO  \n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "# dot_data = StringIO()\n",
        "# # number of unique values in target col\n",
        "# class_names = list(model_frame.categories.unique())\n",
        "\n",
        "# # use trained decision tree model, feature columns, and clases in target col\n",
        "# export_graphviz(dtc, out_file = dot_data, filled=True, rounded=True, special_characters=True,\n",
        "#                 feature_names = features,\n",
        "#                 class_names = class_names)\n",
        "\n",
        "# # creates image and then displays in Jupyter\n",
        "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "# graph.write_png('game_classes.png')\n",
        "# Image(graph.create_png())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}